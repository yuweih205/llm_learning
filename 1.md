个人理解

神经网络就是很多很多函数的模拟和叠加 而权重就是里面的不同函数的参数值 激活值类似与函数的输入值

训练就是把模型确定下来 也就是该模型描述的一种关系集合

推理就是输入首层神经网络的“激活值”直到最后拿到结果

后向传播就是一种梯度下降正向算收敛的补充 因为不可能一次迭代到最低点 通过不同的尝试去逼近真实稳定点

loss函数 评估训练效果 

稳定的解释可能是指某种描述在该空间内最大的可能性 

从数学上来说 就是多维空间的特征提取和映射 还有关系 关系实际就是函数

梯度下降

hessian矩阵 二阶导数 意义？？



自注意力机制

1、如何实现自注意力的

2、归一化函数的选择

3、



bert架构



decoder-only



encoder-decoder



llama





常见方法

稀疏

非结构化稀疏  结构化稀疏 静态稀疏 动态稀疏

量化

压缩

蒸馏

lora

剪枝



常见优化算法

kv cache

qkv融合

gqa

rag—attn



模型部署

PD分离

并行机制



硬件：

脉动矩阵

功耗和尺度的影响

一块电路板上多组件能运行不同功耗





batch梯度下降 使用全部数据集

随机梯度下降

minibatch梯度下降



为啥是mse作为loss function

为啥mlp的隐藏层纬度和attn不一样 它有什么具体的含义吗



序列并行 sp 将输入序列分割成多个子序列 并分配到不同计算节点去计算 跨节点同步

分块流水线并行   将单个请求的输入token分成多个chunk 不同计算节点 kvcahce流式传输到下个节点 有啥区别？？

预填充阶段 prefill  什么叫逐层预填充？？？
