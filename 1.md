个人理解

神经网络就是很多很多函数的模拟和叠加 而权重就是里面的不同函数的参数值 激活值类似与函数的输入值

训练就是把模型确定下来 也就是该模型描述的一种关系集合

推理就是输入首层神经网络的“激活值”直到最后拿到结果

后向传播就是一种梯度下降正向算收敛的补充 因为不可能一次迭代到最低点 通过不同的尝试去逼近真实稳定点

loss函数 评估训练效果 

稳定的解释可能是指某种描述在该空间内最大的可能性 

从数学上来说 就是多维空间的特征提取和映射 还有关系 关系实际就是函数

梯度下降

hessian矩阵 二阶导数 意义？？



自注意力机制

1、如何实现自注意力的

2、归一化函数的选择

3、



bert架构



decoder-only



encoder-decoder



llama





常见方法

稀疏

非结构化稀疏  结构化稀疏 静态稀疏 动态稀疏

量化

压缩

蒸馏

lora

剪枝



常见优化算法

kv cache

qkv融合 忘了

gqa 训练的时候就将kv分组

rag—attn

paged attn 按行分开存储

模型部署

PD分离

并行机制



硬件：

脉动矩阵

功耗和尺度的影响

一块电路板上多组件能运行不同功耗





batch梯度下降 使用全部数据集

随机梯度下降

minibatch梯度下降



为啥是mse作为loss function

为啥mlp的隐藏层纬度和attn不一样 它有什么具体的含义吗



序列并行 sp 将输入序列分割成多个子序列 并分配到不同计算节点去计算 跨节点同步

分块流水线并行   将单个请求的输入token分成多个chunk 不同计算节点 kvcahce流式传输到下个节点 有啥区别？？

预填充阶段 prefill  什么叫逐层预填充？？？

lmhead预估下一个token的概率 忘了



* batch就是不同用户的算子  seq_len是token化后输入序列的长度 训练时可能会被padding到相同长度？（大概是并行gemm计算的要求）n_embd指每个token通过embedding之后的数据维度 所以最开始的输入是多个用户分别输入一句话映射到n维的数据 也就是batch张表

* qkv：

  q：当前 token 想要“查询”什么信息。

  k：当前 token 的存储键，用来被 Query 检索。

  v：实际存储的信息，用于 Attention 加权计算

  llama2采用GQA 这是训练时就做好了的  多组用同样的kv权重

  为什么Q不能共享

* kv_cache

  输入时 batch，seq_len,  embd  矩阵   x   Wk （n_embd， n_embd)(不带gqa)  然后kv_cache 下一个token会在seq_len的维度增加一行 根据矩阵计算的原理 可以单独拿出来 而每次推理下一词都需要此前所有的qkv  所以可以缓存起来 

  没有kv_cache会是计算量平方级增大

* 怎么理解头数呢？ 

​	本质上是为了加速计算来增加的一个超参数 如果单头 理论数据量会变小 但后期可能很难并行计算 或者会引入新的同步操作？在单头tp的时候 也会需要引入同步 从这个角度来说 不同头的数据信息非正交  但是实际表明部分特征会聚集特定部分头（可能是信息维度有限？）

* attn

  ragged_attn 忘了

* qkv融合

* lmhead





